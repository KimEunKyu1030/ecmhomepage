<html>
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=ks_c_5601-1987">
		<link href="./default.css" type="text/css" rel="stylesheet">
		<script type="text/javascript" src="./default.js"></script>
	</head>
	
	<body class="bdstyle" onload="onBodyLoad();" onmousedown="onBodyMouseDown();">
	  <!-- Page Title -->
	  <div id="areafixed" class="bdstyleTitle">
	    <div class="title">DNN
	    <div class="version"></div>
	    </div>
	    <hr class="headerLine">
  	</div>
    <!-- Page Title -->
    
    
    <!-- Begin Client Area -->
    <div id="areascroll" class="bdstyleBody">
	    <p>
    	<div class="node"><img src="./files/DNN.files/DNN_icon.png"></div>
    	<div class="nodeexplain">
			인간 신경의 기본 단위인 뉴런에서 영감을 받은 퍼셉트론을 서로 연결하여 인공신경망을 구성합니다. 신경망의 깊이를 깊게 만든 Deep Neural Network (DNN, 심층신경망) 로 예측/분류 작업을 진행합니다.
    	</div>
    	<div style="clear: both;"></div>

    </p>
    
    <hr class="nodeLine">
    
    <a href="#개요">개요</a>,
    <a href="#고려사항">고려사항</a>,
    <a href="#사용법">사용법</a>,
    <a href="#속성">속성</a>,
    <a href="#결과">결과</a>
		
	
    <a name="개요"><h3>개요</h3></a>

    <ul class="liType02">
		<li>인공신경망 (또는 신경망)은 인간의 신경망이 작동하는 방식에 영감을 얻은 machine learning 방법 중 하나입니다. 퍼셉트론은 인간의 뉴런을 모방한 인공신경망의 기본 단위로, 뉴런이 수상돌기를 통해 신호를 입력 받는 것을 퍼셉트론의 입력으로, 신경세포체에서 threshold에 따라 값을 내보낼지 결정하는 과정을 <span class="genExHighlight">활성 함수</span>로, 축삭말단에서 다음 뉴런으로 신호를 전달하는 것을 퍼셉트론의 출력으로 표현합니다.</li>
		<br><p><img src="./files/DNN.files/DNN_Neurons.png" , style='height: 200px; width: 700px;> object-fit: contain'></p><br>
		<li>인공신경망의 기본 구조는 아래 그림과 같이 여러 <span class="genExHighlight">계층(layers)</span>으로 구성되어 있는 모습이고, 각 계층은 여러 퍼셉트론으로 이루어져 있습니다. 이러한 구성때문에 이 구조는 <span class="genExHighlight">Multi-layer perceptron (MLP)</span> 이라고 불립니다. 기본적인 MLP 구조는 <span class="genExHighlight">입력층(input layer)</span>, 하나 이상의 <span class="genExHighlight">은닉층(hidden layer)</span>, <span class="genExHighlight">출력층(output layer)</span>으로 이루어져 있습니다. 데이터는 입력층으로 받아들여져 하나 이상의 은닉층을 단계적으로 통과하여 출력층으로 전달됩니다. 특히 <span class="genExHighlight">은닉층</span>이 2개 이상인 인공신경망을 <span class="genExHighlight">Deep neural network (DNN)</span> 이라고 합니다.</li>
		<br><p><img src="./files/DNN.files/DNN_structure.png", style='height: 200px; width: 700px; object-fit: contain'></p><br>
		<li>위 그림에서 알 수 있듯이 인접한 두 계층은 <span class="genExHighlight">가중치들(weights)</span>로 연결되어 있습니다. 입력층을 제외한 나머지 계층들은 이전 계층의 모든 퍼셉트론의 출력들과 현재 계층의 모든 퍼셉트론의 입력이 연결되어 있고, 해당 연결에는 고유한 가중치 값을 가지고 있습니다. 즉, 이전 계층의 퍼셉트론 개수가 M개이고 현재 계층의 퍼셉트론 개수가 N개일 경우 가중치는 M×N개 존재합니다. 또한, 현재 계층의 각각의 퍼셉트론이 이전 계층에 영향에 받지 않는 별도의 값을 표현하기 위해 bias값이 포함됩니다. 가중치를 설명할 때 이 bias 값을 포함하여 표현할 수도 있고 이런 경우 가중치의 크기는 각각의 현재 계층 별로 bias가 하나씩 포함되므로 (M+1)×N개가 됩니다.</li>
		<li><span class="genExHighlight">인공신경망</span>에서는 가중치가 함수의 출력을 결정해주는 변수입니다. 즉, 가중치에 따라 독립변수를 넣었을 때 예측하는 결과 값이 올바르게 나올 수도 있고 틀리게 나올 수도 있습니다. 따라서 인공신경망이 제대로 된 값을 예측하도록 만들어내는 학습 과정은 <span class="genExHighlight">최적의 가중치 조합</span>을 찾아내는 과정입니다. </li>
		<li><span class="genExHighlight">DNN</span>에서는 이러한 최적화를 진행하기 위해 <span class="genExHighlight">gradient descent (경사하강법)</span> 기반 알고리즘을사용합니다. 아래 그림의 (a)에서 시작 지점의 gradient 값을 구하면 (1차원인 경우 미분 값 또는 해당 지점의 기울기) 음수의 값이 나옵니다. 이 경우 가중치는 gradient의 반대 방향으로 이동시켜야 손실함수가 감소하는 방향이 됩니다. 반대로 (c)에서는 gradient가 양수 값이므로 가중치는 음수방향으로 이동하여야 손실함수가 이 감소합니다. Gradient 방향 뿐만 아니라 크기도 중요합니다. 아래 그림 (a)의 시작점은 그림 4 (b)의 시작점보다 멉니다. 일반적으로 최소 지점에서 더욱 멀리 있는 지점의 gradient 크기는 가까이 있는 지점보다 크기가 클 가능성이 높기 때문에 gradient가 클 수록 많이 이동하여야 합니다. </li>
		<br><p><img src="./files/DNN.files/DNN_Gradient.png", style='height: 200px; width: 700px; object-fit: contain'></p><br>
		<li>W 는 인공신경망의 모든 가중치 와 bias 를 원소로 하는 값 배열, C(W) 는 최소화를 할 대상이 되는 손실함수, ∇_W F(W) 는 W에 대한 함수 F(W)의 gradient, α 는 학습 계수</li>
		<li>정리해보면 가중치 W는 gradient 반대 방향으로 이동하여야 하고, gradient의 크기에 비례하여 이동하여야 합니다.</li>
 </li>
  	</ul>
  	
  	<a name="고려사항"><h3>고려사항</h3></a>
    <ul class="liType02">
  		<li><span class="genExHighlight">독립변수</span>는 <span class="genExHighlight">연속형</span> 변수만 사용 가능합니다.</li>
		<li><span class="genExHighlight">종속변수</span>는 <span class="genExHighlight">연속형/이산형</span> 모두 사용 가능합니다. 종속변수에 이산형이 입력될 경우 One-Hot Encoding이 진행됩니다.</li>
  	</ul>
  	
  	<a name="사용법"><h3>사용법</h3></a>
    <ul class="liType02">
  		<li><span class="genExHighlight">입력노드</span>를 통해 데이터를 읽어들입니다.</li>
  		<li><span class="genExHighlight">형태 변경 노드</span>를 통해 읽어들인 데이터의 타입을 지정합니다.(<span class="genExHighlight">독립변수, 종속변수</span>를 지정)</li>
  		<li><span class="genExHighlight">DNN 노드</span>를 <span class="genExHighlight">형태 변경 노드</span>에 연결하고 옵션들을 선택합니다.</li>
  		<li><span class="genExHighlight">화면표시 노드</span>를 <span class="genExHighlight">DNN 노드</span>에 연결합니다. </li>
		<li><span class="genExHighlight">DNN diagram</span> 예시는 아래와 같습니다.</li>
  		<p>			
    	<img src="./files/DNN.files/DNN_diagram.png">
  		</p>
  	</ul>
			
		
    <a name="속성"><h3>속성</h3></a>
		<table class="tbStyle01">
			<thead>
				<tr>
					<th width="60">속성그룹</th>
					<th width="100">속성명</th>
					<th width="">설명</th>
					<th width="100">기타</th>
					<th width="60">비고</th>
				</tr>
			</thead>
			
				<tr>
					<th rowspan="2">일반정보</th>
					<td>이름</td>
					<td>노드의 이름을 입력합니다.</td>
					<td>선택</td>
					<td></td>
				</tr>
				<tr>
					<td>설명</td>
					<td>노드에 대한 간단한 주석을 달 수 있습니다.</td>
					<td>선택</td>
					<td></td>
				</tr>
				<tr>
					<th rowspan="2">모델파일</th>
					<td>모델파일 생성</td>
					<td>모델링 후에 모델 파일을 생성할지의 여부를 선택합니다.</td>
					<td>필수</td>
					<td>예, 아니오</td>
				</tr>
				<tr>
					<td>모델파일 경로</td>
					<td>모델 파일을 생성할시 저장할 모델 파일의 경로를 선택합니다.</td>
					<td>조건부 필수</td>
					<td></td>
				</tr>
				<tr>
					<th rowspan="11">선택사항</th>
					<td>시행수</td>
					<td>데이터셋을 최대 몇 회 학습할지 선정합니다.</td>
					<td>필수</td>
					<td>자연수</td>
				</tr>
				<tr>
					<td>최종목표에러</td>
					<td>학습이 끝나기 위한 최대 에러 조건을 선정합니다. </td>
					<td>필수</td>
					<td>0이상의 실수</td>
				</tr>
				<tr>
					<td>손실함수</td>
					<td>신경망 출력층의 값과 목표 값을 비교하는 손실함수를 선택합니다.</td>
					<td>필수</td>
					<td>Sum Of Square Error, Cross Entropy</td>
				</tr>
				<tr>
					<td>출력층 활성함수</td>
					<td>손실함수가 Sum of Squared Error인 경우신경망 출력층의 활성함수를 선택합니다. 손실함수가 Cross Entropy인 경우 활성함수 대신 SoftMax가 적용됩니다. </td>
					<td>조건부 필수</td>
					<td>Linear, Tanh, LogSig, ReLU, Leaky ReLU, ELU</td>
				</tr>
				<tr>
					<td>최적화 기법</td>
					<td>최적화 기법을 선택합니다.</td>
					<td>필수</td>
					<td>Gradient Descent with Momentum, AdaGrad, RMSProp, ADAM</td>
				</tr>
				<tr>
					<td>배치크기</td>
					<td>Mini-batch gradient descent에서 한번에 업데이트할 데이터 수를 설정합니다.</td>
					<td>필수</td>
					<td>1이상 전체 입력 데이터 수 이하의 정수</td>
				</tr>
				<tr>
					<td>학습률</td>
					<td>최적화 기법 도중 가중치를 업데이트 하는 정도를 설정합니다.</td>
					<td>필수</td>
					<td>0초과 1이하의 실수</td>
				</tr>
				<tr>
					<td>모멘텀</td>
					<td>Gradient descent with momentum 최적화 기법에서 가중치가 기존에 이동하는 방향을 반영하는 정도를 설정합니다.</td>
					<td>조건부 필수</td>
					<td>0이상 1미만의 실수</td>
				</tr>
				<tr>
					<td>Gamma</td>
					<td>RMSProp 최적화 기법에서 기존에 업데이트된 가중치를 기억하는 비율을 설정합니다.</td>
					<td>조건부 필수</td>
					<td>0이상 1미만의 실수</td>
				</tr>
				<tr>
					<td>Beta1</td>
					<td>ADAM 최적화 기법에서 가중치가 기존에 이동하는 방향을 반영하는 정도를 설정합니다.</td>
					<td>조건부 필수</td>
					<td>0이상 1미만의 실수</td>
				</tr>
				<tr>
					<td>Beta2</td>
					<td>ADAM 최적화 기법에서 기존에 업데이트된 가중치를 기억하는 비율을 설정합니다.</td>
					<td>조건부 필수</td>
					<td>0이상 1미만의 실수</td>
				</tr>				
				<tr>
					<th rowspan="2">선택사항-은닉층 설정</th>
					<td>은닉층 추가</td>
					<td>인공신경망의 은닉층을 추가합니다.</td>
					<td>버튼</td>
					<td></td>
				</tr>
				<tr>
					<td>마지막 은닉층 제거</td>
					<td>가장 마지막에 추가된 인공신경망을 제거합니다.</td>
					<td>버튼</td>
					<td></td>
				</tr>
					<th rowspan="2">선택사항-은닉층 설정-은닉층#</th>
					<td>노드 개수</td>
					<td>은닉층 #의 퍼셉트론 개수를 설정합니다.</td>
					<td>필수</td>
					<td>자연수</td>
				</tr>
				<tr>
					<td>활성함수</td>
					<td>은닉층 #의 활성함수를 선택합니다.</td>
					<td>필수</td>
					<td>Linear, TanSig, LogSig, ReLU, Leaky ReLU, ELU</td>
				</tr>
		</table>
		<ul class="liType02">
			<li><span class="genExHighlight">활성화 함수</span></li>
			<p><img src="./files/DNN.files/DNN_Activation.png", style='height: 370px; width: 800px; object-fit: contain'></p><br>
			<li><span class="genExHighlight">배치 크기</span></li>
			1)	Gradient Descent with Momentum
			움직이는 물체가 같은 방향으로 계속 움직이려 하는 관성의 법칙과 유사한 방법으로 가중치를 업데이트 합니다. 모멘텀(Momentum) 계수가 1에 가까울 수록 기존 이동 방향을 유지하려는 성향을 가지고 0에 가까울 수록 일반 gradient descent에 가까워집니다. 극단적으로 모멘텀 계수가 1인 경우는 기존의 방향을 계속 유지하면서 새로운 gradient를 추가합니다. 반대로 모멘텀 계수가 0인 경우 일반 gradient descent와 같아집니다.
			<li><span class="genExHighlight">최적화 알고리즘</span></li>
			<span class="genExHighlight">1)	Gradient Descent with Momentum</span><br>
			움직이는 물체가 같은 방향으로 계속 움직이려 하는 관성의 법칙과 유사한 방법으로 가중치를 업데이트 합니다. 모멘텀(Momentum) 계수가 1에 가까울 수록 기존 이동 방향을 유지하려는 성향을 가지고 0에 가까울 수록 일반 gradient descent에 가까워집니다. 극단적으로 모멘텀 계수가 1인 경우는 기존의 방향을 계속 유지하면서 새로운 gradient를 추가합니다. 반대로 모멘텀 계수가 0인 경우 일반 gradient descent와 같아집니다.<br>
			<span class="genExHighlight">2)	Adagrad</span><br>
			업데이트 되는 변수 중 학습 과정에서 변동이 많은 변수들에 대한 학습을 줄입니다. 예를 들어 변수 1이 매 업데이트 과정에서 10, 10, 10씩 업데이트 되었고 변수 2가 1,1,1씩 업데이트되었다고 하면 변수 1 방향으로 업데이트가 많이 된 것이므로 변수 1에대한 학습 세기를 줄입니다. 반대의 경우 학습 세기를 늘립니다.<br>
			<span class="genExHighlight">3)	RMSProp</span><br>
			Adagrad는 가변적 학습 세기를 적용하여 여러 변수에서 업데이트가 골고루 되게 하는 장점을 가지고 있지만, 업데이트시 오래 전의 gradient의 크기까지 기억하기 때문에 현재 업데이트의 학습 세기가 0에 가까워지는 문제가 발생하기도 합니다. 이를 해결하기 위한 방법이 RMSProp입니다. RMSProp은 이때까지 계산한 gradient를 모두 동등하게 반영하여 학습세기를 조절하는 것이 아니라 최근에 발생한 gradient의 중요도를 높게 반영하여 학습세기를 조절합니다. RMSProp에서 gamma 값은 개별적인 변수에 대해 기존의 gradient의 크기 기록을 저장한 값과 현재의 gradient 크기를 몇대 몇으로 반영하여 학습세기를 조절할지 결정하는 변수입니다. Gamma값이 1에 가까우면 기존의 오래된 gradient의 크기도 학습률 조절에 반영이 됩니다. 예를 들어 어떤 변수의 업데이트 시점 별 편미분 값이 {100, 0, 0}인 경우 gamma 값이 클 수록 편미분값 100을 오랫동안 기억하여 학습세기를 작게 조절합니다. 반면에 gamma 값이 작을수록 기존에 학습된 편미분 값 100을 빨리 잊어버려 학습세기를 크게 조절합니다. <br>
			<span class="genExHighlight">4)	ADAM</span><br>
			ADAM은 기존의 가중치의 이동 진행방향을 유지하는 momentum의 장점과, 학습이 많이 진행된 변수에 대한 학습세기를 줄여주는 RMSProp의 장점을 동시에 사용하기 위해 개발된 학습 방법입니다. 설정 가능한 옵션은 beta1과 beta2가 있습니다. Beta1값은 momentum 계수와 같이 1에 가까울 수록 기존의 gradient값이 이동하려는 방향성을 유지시켜줍니다. Beta2 값은 RMSProp의 gamma 값과 유사하게 현재시점 이전의 gradient의 크기를 유지하는 비율입니다Beta2 값이 1에 가까울수록, 이전 업데이트 시점에 의해 현재 시점의 학습세기가 줄어드는 영향력이 늘어납니다.<br>
		</ul>
		<a name="결과"><h3>결과</h3></a>
        <ul class="liType02">
		<li><span class="genExHighlight">메시지 창</span></li>
		메시지 창에 매 10번째 iteration마다 RMSE 값을 띄워줍니다.
		<p><img src="./files/DNN.files/DNN_Message.png", style='height: 250px; width: 600px; object-fit: contain'></p><br>
  		<li><span class="genExHighlight">분석결과정보</span></li>			
    	<span class="genExHighlight">화면표시</span> 노드에서 종속변수의 형태에 따라 분류/예측 결과를 확인할 수 있습니다. 아래 그림에서 학습 시 연속형 종속변수 A1를 DNN으로 예측한 결과입니다. 변수 DNN#_YHAT1은 (#는 생성된 노드에 따라 변동) A1 종속변수를 예측하는 예측 값입니다. 변수 DNN#_Residual1은 A1 종속변수와 예측값의 차이 입니다. 
		<p><img src="./files/DNN.files/DNN_result1.png", style='height: 370px; width: 800px; object-fit: contain'></p><br>
  	</ul>
		
		<hr class="footerLine">
		<p class="footer">
	</div>
	</body>
</html>
